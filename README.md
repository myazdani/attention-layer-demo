# Understanding the Attention layer with a toy sequential task

The attention layer has been adopted in numerous sequential and spatial processing. David Blei characterized the attention layer as highlighting features that are relevant for predicting and outcome can depend on the entire feature set as well. 

François Fleuret came up with a toy sequential task that I find really helpful for playing with such attention mechanisms: https://twitter.com/francoisfleuret/status/1262639062785105922

In this repo I take François' work and explore it in a notebook setting. I find notebooks to be the ideal playground for fiddling with ideas and his code serves as a template to be fiddled with. 

The `demo.ipynb` notebook goes through his code and tries to explain how the various components work. I tried to strip away his original code to what is most useful for learning and exploring. 
